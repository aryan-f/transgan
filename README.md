# TransGAN

![TransGAN](https://github.com/VITA-Group/TransGAN/blob/master/assets/TransGAN_1.png)

Jiang, Yifan, Shiyu Chang, and Zhangyang Wang. "[TransGAN: Two Pure Transformers Can Make One Strong GAN, and That Can Scale Up](https://proceedings.neurips.cc/paper/2021/hash/7c220a2091c26a7f5e9f1cfb099511e3-Abstract.html)." Advances in Neural Information Processing Systems 34 (2021): 14745-14758.

The notebook is based on the [publically available implementation](https://github.com/VITA-Group/TransGAN), but without all the unnecessary bells and whistles. Furthermore, we go over every intricet detail of the model and the learning algorithm mentioned in the paper and implement them out one by one.

Note that you need to restart your kernel *many many times* if you are going to train it on Free-tier Colab. Make sure to use the checkpointing functionality in the notebook. You can find some my latest checkpoints in the **Releases** section.
